{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country wise Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# import threading\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# # Connecting with Google Sheets\n",
    "# import gspread\n",
    "# from df2gspread import df2gspread as d2g\n",
    "# from df2gspread import gspread2df as g2d\n",
    "\n",
    "# from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# # Geo-Parsing the text data\n",
    "\n",
    "# import geopandas as gpd\n",
    "# from geotext import GeoText\n",
    "\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # %matplotlib inline\n",
    "# # from geopy.geocoders import Nominatim\n",
    "# # from geopy.exc import GeocoderTimedOut\n",
    "\n",
    "# # download nltk dependent files : punkt, stopwords, averaged_perceptron_tagger, maxent_ne_chunker, words\n",
    "# import nltk\n",
    "# from nltk import load_parser\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "# from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ServiceAccountCredentials' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Name of our Service Account Key\u001b[39;00m\n\u001b[1;32m      5\u001b[0m google_key_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../TwitterAppData/twitterscrapeddata-73e2d5298a3a.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m credentials \u001b[38;5;241m=\u001b[39m \u001b[43mServiceAccountCredentials\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_json_keyfile_name(google_key_file, scope)\n\u001b[1;32m      7\u001b[0m gc \u001b[38;5;241m=\u001b[39m gspread\u001b[38;5;241m.\u001b[39mauthorize(credentials)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# This is the Worksheet ID - CountryScrape\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ServiceAccountCredentials' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Default Scope\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Name of our Service Account Key\n",
    "google_key_file = '../../TwitterAppData/twitterscrapeddata-73e2d5298a3a.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "\n",
    "# This is the Worksheet ID - CountryScrape\n",
    "spreadsheet_key = '1692ntzc3LTMeEqHYSgV3KGbSGSNEiycqPAl03ZNPMiQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../TwitterAppData/twiterAppData_CountryScrape.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m twitterAppCredentials \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../TwitterAppData/twiterAppData_CountryScrape.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m consumer_key \u001b[38;5;241m=\u001b[39m twitterAppCredentials[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsumer_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m consumer_secret \u001b[38;5;241m=\u001b[39m twitterAppCredentials[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsumer_secret\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../TwitterAppData/twiterAppData_CountryScrape.json'"
     ]
    }
   ],
   "source": [
    "twitterAppCredentials = json.load(open(\"../../TwitterAppData/twiterAppData_CountryScrape.json\", \"r\"))\n",
    "\n",
    "consumer_key = twitterAppCredentials[\"consumer_key\"]\n",
    "consumer_secret = twitterAppCredentials[\"consumer_secret\"]\n",
    "access_token = twitterAppCredentials[\"access_token\"]\n",
    "access_token_secret = twitterAppCredentials[\"access_token_secret\"]\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_indexes = [\"created_at\",\n",
    "               \"id_str\", \"user_id\",\n",
    "               \"is_quote_status\",\n",
    "               \"text\", \"verified\", \n",
    "               \"likes\", \"retweet_count\",\n",
    "               \"location\",\"json\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text, label):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree and subtree.label() == label:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "            \n",
    "        if current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUnTaggedLocation(userLoc, text):\n",
    "    flag_user = 0\n",
    "    loc = {}\n",
    "    \n",
    "    if not(userLoc is np.nan):\n",
    "        #user_loc not null\n",
    "        userLoc = re.split(\",|;|/|\\n\",userLoc)\n",
    "        flag_user = 1\n",
    "    \n",
    "    flag_text = 1\n",
    "    textLoc = get_continuous_chunks(text,'GPE')\n",
    "    if len(textLoc) == 0:\n",
    "        textLoc = None\n",
    "        flag_text = 0\n",
    "\n",
    "        \n",
    "    if (flag_user*flag_text != 0):     # Neither user nor Text is empty\n",
    "        textLoc.extend(userLoc)\n",
    "        loc = set(textLoc)\n",
    "        \n",
    "    elif(flag_user == 0)&(flag_text == 0):   # Both user and Text is empty\n",
    "        return None\n",
    "        \n",
    "    else:                                  # Either user or Text is empty\n",
    "        if (flag_user == 1):                          # User NOT empty\n",
    "            loc = userLoc\n",
    "        else:                                         # Text NOT Empty\n",
    "            loc = textLoc\n",
    "            \n",
    "    out = []\n",
    "    for i in loc:\n",
    "        out.append(''.join(list(GeoText(i).country_mentions.keys())))\n",
    "    out = set(out)\n",
    "    if '' in out:\n",
    "        out.remove('')\n",
    "        if len(out) == 0:\n",
    "            out = None\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTaggedLocation(tweetPlace):\n",
    "    if tweetPlace is not None:\n",
    "#         print(tweetPlace[\"full_name\"],tweetPlace[\"country_code\"])\n",
    "        tweet_loc = set([tweetPlace[\"full_name\"],tweetPlace[\"country_code\"]])\n",
    "        return tweet_loc\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnsembleLoc(geoTaggedLoc, userLoc, text):\n",
    "    if geoTaggedLoc is np.nan:\n",
    "        return getTaggedLocation\n",
    "    else:\n",
    "        return getUnTaggedLocation(userLoc, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetStructuredTweet(inp):\n",
    "    try:\n",
    "        created_at = inp[\"created_at\"]\n",
    "        id_str = inp[\"id_str\"]\n",
    "        user_id = inp[\"user\"][\"id_str\"]\n",
    "        is_quote_status = inp[\"is_quote_status\"]\n",
    "        \n",
    "        text = inp[\"extended_tweet\"][\"full_text\"] if \"extended_tweet\" in inp else inp[\"text\"]\n",
    "    \n",
    "        likes = inp[\"favorite_count\"]\n",
    "        retweet_count = inp[\"retweet_count\"]\n",
    "        verified = inp[\"user\"][\"verified\"]\n",
    "        json_body = json.dumps(inp)\n",
    "\n",
    "        user_loc = inp[\"user\"][\"location\"] or np.nan\n",
    "        \n",
    "        location = getEnsembleLoc(inp[\"place\"], user_loc, text)\n",
    "        \n",
    "        if(location != None):\n",
    "            location = ','.join(list(location))\n",
    "        \n",
    "        k = pd.Series([created_at,\n",
    "                       id_str, user_id,\n",
    "                       is_quote_status,\n",
    "                       text, verified,\n",
    "                       likes, retweet_count,\n",
    "                       location, json_body\n",
    "                      ],\n",
    "                 index = col_indexes).to_frame().T\n",
    "        return k\n",
    "    except BaseException as ex:\n",
    "        print('failed in Structuring the tweet: ',inp[\"id_str\"],ex)\n",
    "        return pd.DataFrame(columns = col_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Manage_ID:\n",
    "    \n",
    "    def __init__(self, location):\n",
    "        try:\n",
    "            self.filePath = \"../data/ID/idSet_\"+location\n",
    "            print(self.filePath)\n",
    "            self.id_set = set(pickle.load(open(self.filePath,\"rb\")))\n",
    "        except:\n",
    "            self.id_set = set()\n",
    "            print(\"Input File Empty\")\n",
    "        print(\"begin->\", len(self.id_set))\n",
    "        return\n",
    "    \n",
    "    def __del__(self):\n",
    "        print(\"end->\", len(self.id_set))\n",
    "        pickle.dump(self.id_set, open(self.filePath, \"wb\"))\n",
    "        return\n",
    "        \n",
    "    def CheckID(self,inputID):      # Flag = True -> already Exists\n",
    "        return inputID in self.id_set\n",
    "    \n",
    "    def UpdateID(self, inputID):\n",
    "        self.id_set.add(inputID)\n",
    "        return\n",
    "        \n",
    "    def GetLen(self):\n",
    "        return len(self.id_set)\n",
    "        \n",
    "    def OutputID(self):\n",
    "        for i in self.id_set:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetFilter():\n",
    "    def __init__(self, location, bufferSize):\n",
    "        self.status = ''\n",
    "        self.structuredTweet = ''\n",
    "        self.idSet = Manage_ID(location)\n",
    "        \n",
    "        self.rowCount = self.idSet.GetLen()+1             #Change to ensure data not overriden\n",
    "        self.isFirstUpload = True                         # Flag to identify is First upload( of an already existing idSet). \n",
    "                                                          # Set to False if idSet is newly made!\n",
    "            \n",
    "        self.location = location\n",
    "        print(\"Uploading to Worksheet: \"+location)\n",
    "        \n",
    "        self.failed_tweets = []\n",
    "        self.buffer_size = bufferSize\n",
    "        \n",
    "        self.tweetsList = pd.DataFrame(columns = col_indexes)\n",
    "        return\n",
    "    \n",
    "    def __del__(self):\n",
    "        print(\"Closing Uploads, Clearing Buffer for \"+self.location)\n",
    "        self.UploadTweet()\n",
    "        del self.idSet\n",
    "        \n",
    "    \n",
    "    def FailPreFilters(self, data):\n",
    "        flag = False                # Assume every tweet passes pre-filters by default; untill proven otherwise\n",
    "        # Add Duplication Check\n",
    "        flag = self.idSet.CheckID(data[\"id_str\"])    # Flag = True -> already Exists\n",
    "        \n",
    "        return flag\n",
    "    \n",
    "    \n",
    "    def FailPostFilters(self, data):\n",
    "        flag = False                # Assume every tweet passes post-filters by default; untill proven otherwise\n",
    "        # Add Country Check\n",
    "        try:\n",
    "            if self.location not in data[\"location\"][0]:\n",
    "                flag = True\n",
    "        except:\n",
    "            flag = True\n",
    "        \n",
    "        return flag\n",
    "    \n",
    "    \n",
    "    def UploadTweet(self):\n",
    "        try:\n",
    "            recordCols = False\n",
    "            if self.rowCount == 1:\n",
    "                self.isFirstUpload = False\n",
    "                recordCols = True\n",
    "                \n",
    "            if self.isFirstUpload:\n",
    "                self.rowCount = self.rowCount + 1  # To cater for the Header\n",
    "                self.isFirstUpload = False\n",
    "            \n",
    "            x = d2g.upload(self.tweetsList,\n",
    "                       spreadsheet_key,\n",
    "                       self.location,                 # Country Name = WorksheetName\n",
    "                       credentials=credentials,\n",
    "                       col_names=recordCols,\n",
    "                       row_names=True,\n",
    "                       start_cell = ''.join(['A',str(self.rowCount)]),  \n",
    "                       clean=False)\n",
    "            \n",
    "            print(\"Uploading to WorkSheet: \"+self.location,end=\" : \")\n",
    "            print(str(len(self.tweetsList))+\" observations starting from cell: \"+''.join(['A',str(self.rowCount)]))\n",
    "\n",
    "            if recordCols:\n",
    "                self.rowCount = self.rowCount + self.buffer_size +1\n",
    "            else:\n",
    "                self.rowCount = self.rowCount + self.buffer_size\n",
    "            self.tweetsList = self.tweetsList.iloc[0:0]\n",
    "            return True\n",
    "        except BaseException as ex:\n",
    "            print('failed while Uploading to Google Sheets: ',str(ex))\n",
    "            self.failed_tweets.append(self.status[\"id_str\"])\n",
    "            return False\n",
    "\n",
    "    def StoreTweet(self, data):\n",
    "        # Store Tweets and Upload Them\n",
    "        try:\n",
    "            if self.idSet.CheckID(data[\"id_str\"][0]):\n",
    "                return False\n",
    "            \n",
    "            self.idSet.UpdateID(data[\"id_str\"][0])\n",
    "            \n",
    "            self.tweetsList = pd.concat([self.tweetsList, data], ignore_index=True)\n",
    "            if self.tweetsList.shape[0] >= self.buffer_size:\n",
    "                status = self.UploadTweet()\n",
    "            return True\n",
    "        \n",
    "        except BaseException as ex:\n",
    "            print('failed in UploadTweet: ',str(ex))\n",
    "            self.failed_tweets.append(self.status[\"id_str\"])\n",
    "            return False\n",
    "        \n",
    "    def ProcessTweet(self):\n",
    "        try:\n",
    "            # StructureTweet()\n",
    "            structuredTweet = GetStructuredTweet(self.status)\n",
    "\n",
    "            # Discard Tweet if Post-Filter Criterias not met\n",
    "            if self.FailPostFilters(structuredTweet): \n",
    "                return False\n",
    "\n",
    "            # StoreTweet()\n",
    "            status = self.StoreTweet(structuredTweet)\n",
    "            return status\n",
    "\n",
    "        except BaseException as ex:\n",
    "            print('failed: ',str(ex))\n",
    "            self.failed_tweets.append(self.status[\"id_str\"])\n",
    "            return False\n",
    "        \n",
    "    def PreProcessTweet(self, tweet):\n",
    "        body = ''\n",
    "        #Quoted\n",
    "        if \"quoted_status\" in tweet._json:\n",
    "            if \"retweeted_status\" in tweet._json[\"quoted_status\"]:\n",
    "                body = tweet._json[\"quoted_status\"][\"retweeted_status\"]\n",
    "            else:\n",
    "                body = tweet._json[\"quoted_status\"]\n",
    "        \n",
    "        #Retweeted\n",
    "        elif \"retweeted_status\" in tweet._json:\n",
    "            if \"quoted_status\" in tweet._json[\"retweeted_status\"]:\n",
    "                body = tweet._json[\"retweeted_status\"][\"quoted_status\"]\n",
    "            else:\n",
    "                body = tweet._json[\"retweeted_status\"]\n",
    "            \n",
    "        #Original\n",
    "        else:\n",
    "            body = tweet._json\n",
    "\n",
    "        # Discard Tweet if Pre-Filter Criterias not met\n",
    "        if self.FailPreFilters(body):\n",
    "            return False\n",
    "        #else\n",
    "        self.status = body\n",
    "        self.ProcessTweet()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#override tweepy.StreamListener to add logic to on_status\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "\n",
    "    def __init__(self, location, bufferSize):\n",
    "        super().__init__()\n",
    "        self.dataFilter = TweetFilter(location, bufferSize)\n",
    "        \n",
    "    def on_status(self, status):\n",
    "        self.dataFilter.PreProcessTweet(status)\n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        print(status_code)\n",
    "        if status_code == 420:\n",
    "            #returning False in on_error disconnects the stream\n",
    "            return False\n",
    "        if status_code == 406:\n",
    "            print(\"on_error code: 406\")\n",
    "            #returning False in on_error disconnects the stream\n",
    "            return False\n",
    "        \n",
    "    def on_exception(self, exception):\n",
    "        print(exception)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountryScraper:\n",
    "    def __init__(self, topicList, location, buffer_size):\n",
    "        self.countryListener = MyStreamListener(location, buffer_size)\n",
    "        self.countryStream = tweepy.Stream(auth = api.auth, listener=self.countryListener)\n",
    "        return\n",
    "\n",
    "    def BeginStreaming(self):\n",
    "        self.countryStream.filter(track=topicList, languages = [\"en\"], is_async=True)\n",
    "        return\n",
    "    \n",
    "    def EndStreaming(self):\n",
    "        self.countryStream.disconnect()\n",
    "        return\n",
    "    \n",
    "    def GetDetails(self):\n",
    "        print(self.countryListener.dataFilter.tweetsList.shape)\n",
    "        print(\"Failed Tweets:\\n\",self.countryListener.dataFilter.failed_tweets)\n",
    "        return self.countryListener.dataFilter.tweetsList\n",
    "    \n",
    "    def __del__(self):\n",
    "        del self.countryListener\n",
    "        del self.countryStream\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buffer_size = 200\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    topicList = ['blackLivesMatter']\n",
    "    countryList = [\"US\"]\n",
    "    countryScrape = {}\n",
    "    \n",
    "    for country in countryList:\n",
    "        countryScrape[country] = CountryScraper(topicList, country, buffer_size)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if(countryScrape[country] == False):\n",
    "            print(\"Discarding Instance for \"+country)\n",
    "            countryScrape.remove(country)\n",
    "        else:\n",
    "            countryScrape[country].BeginStreaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countryScrape[\"GB\"].GetDetails().tail(7)\n",
    "# countryScrape[\"IN\"].GetDetails().tail(7)\n",
    "countryScrape[\"US\"].GetDetails().tail(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in countryScrape.keys():\n",
    "    print(\"Stopping Scraping for: \",location)\n",
    "    countryScrape[location].EndStreaming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del countryScrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reconciling IDSets\n",
    "# wks_name = \"US\"\n",
    "# df2 = g2d.download(spreadsheet_key, wks_name, credentials=credentials, col_names = True, row_names = True)\n",
    "# allIDs = set(df2[\"id_str\"])\n",
    "# print(len(allIDs))\n",
    "# pickle.dump(allIDs, open(\"../data/ID/idSet_\"+wks_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
